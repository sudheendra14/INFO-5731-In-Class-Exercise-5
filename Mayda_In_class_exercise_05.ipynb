{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Mayda In-class-exercise-05.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMMz8m6ttNG3"
      },
      "source": [
        "## The fifth In-class-exercise (2/23/2021, 20 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cezzInJStNG8"
      },
      "source": [
        "In exercise-03, I asked you to collected 500 textual data based on your own information needs (If you didn't collect the textual data, you should recollect for this exercise). Now we need to think about how to represent the textual data for text classification. In this exercise, you are required to select 10 types of features (10 types of features but absolutely more than 10 features) in the followings feature list, then represent the 500 texts with these features. The output should be in the following format:\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "The feature list:\n",
        "\n",
        "* (1) tf-idf features\n",
        "* (2) POS-tag features: number of adjective, adverb, auxiliary, punctuation, complementizer, coordinating conjunction, subordinating conjunction, determiner, interjection, noun, possessor, preposition, pronoun, quantifier, verb, and other. (select some of them if you use pos-tag features)\n",
        "* (3) Linguistic features:\n",
        "  * number of right-branching nodes across all constituent types\n",
        "  * number of right-branching nodes for NPs only\n",
        "  * number of left-branching nodes across all constituent types\n",
        "  * number of left-branching nodes for NPs only\n",
        "  * number of premodifiers across all constituent types\n",
        "  * number of premodifiers within NPs only\n",
        "  * number of postmodifiers across all constituent types\n",
        "  * number of postmodifiers within NPs only\n",
        "  * branching index across all constituent types, i.e. the number of right-branching nodes minus number of left-branching nodes\n",
        "  * branching index for NPs only\n",
        "  * branching weight index: number of tokens covered by right-branching nodes minus number of tokens covered by left-branching nodes across all categories\n",
        "  * branching weight index for NPs only \n",
        "  * modification index, i.e. the number of premodifiers minus the number of postmodifiers across all categories\n",
        "  * modification index for NPs only\n",
        "  * modification weight index: length in tokens of all premodifiers minus length in tokens of all postmodifiers across all categories\n",
        "  * modification weight index for NPs only\n",
        "  * coordination balance, i.e. the maximal length difference in coordinated constituents\n",
        "  \n",
        "  * density (density can be calculated using the ratio of folowing function words to content words) of determiners/quantifiers\n",
        "  * density of pronouns\n",
        "  * density of prepositions\n",
        "  * density of punctuation marks, specifically commas and semicolons\n",
        "  * density of auxiliary verbs\n",
        "  * density of conjunctions\n",
        "  * density of different pronoun types: Wh, 1st, 2nd, and 3rd person pronouns\n",
        "  \n",
        "  * maximal and average NP length\n",
        "  * maximal and average AJP length\n",
        "  * maximal and average PP length\n",
        "  * maximal and average AVP length\n",
        "  * sentence length\n",
        "\n",
        "* Other features in your mind (ie., pre-defined patterns)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVrXL1FztNHA"
      },
      "source": [
        "# Please write your code here\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "import requests\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "from textblob import Word\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "Fa2KBmBJt0Zg",
        "outputId": "ca9cb5aa-e664-4df9-eecc-f8c943b137eb"
      },
      "source": [
        "text = []\r\n",
        "first_page = 'https://citeseerx.ist.psu.edu/search;jsessionid=37A0AC54277865A394D5F96748080D17?q=natural+language+processing&t=doc&sort=date'\r\n",
        "for page_num in range(50):\r\n",
        "  if page_num == 0:\r\n",
        "    website_link = first_page\r\n",
        "  else:\r\n",
        "    website_link = 'https://citeseerx.ist.psu.edu/search?q=natural+language+processing&t=doc&sort=date&start=' + str (page_num)\r\n",
        "  page = requests.get(website_link)\r\n",
        "  soup = BeautifulSoup(page.text, 'html.parser')\r\n",
        "  abstracts = soup.find_all(class_='pubabstract')\r\n",
        "  page_num = page_num + 10\r\n",
        "  for abstract in abstracts:\r\n",
        "    processed_text = abstract.text.replace('\\n', '').strip()\r\n",
        "    text.append(processed_text)\r\n",
        "data_frame = pd.DataFrame((text), columns =['NLP Text'])\r\n",
        "data_frame"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NLP Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>to process knowledge stored in distributed het...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Abstract: The Unified Modeling Language (UML) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-fluent approach to conflict means working ove...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the critical nature of CMP as described in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>strength. Numerous studies are present that sh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>FD-buffer:\\t a\\tbuffer\\tmanager\\tfor\\tdatabase...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>ABSTRACT The adsorption kinetics of pure N 2 O...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>Abstract Despite low attention level in Wester...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>Abstract Networked learning happens naturally ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>the use of teaching aids. The use of teaching ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows Ã— 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              NLP Text\n",
              "0    to process knowledge stored in distributed het...\n",
              "1    Abstract: The Unified Modeling Language (UML) ...\n",
              "2    -fluent approach to conflict means working ove...\n",
              "3    the critical nature of CMP as described in the...\n",
              "4    strength. Numerous studies are present that sh...\n",
              "..                                                 ...\n",
              "495  FD-buffer:\\t a\\tbuffer\\tmanager\\tfor\\tdatabase...\n",
              "496  ABSTRACT The adsorption kinetics of pure N 2 O...\n",
              "497  Abstract Despite low attention level in Wester...\n",
              "498  Abstract Networked learning happens naturally ...\n",
              "499  the use of teaching aids. The use of teaching ...\n",
              "\n",
              "[500 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXe8uHGQua7Y"
      },
      "source": [
        "data_frame['Special Characters Removal'] = data_frame['NLP Text'].apply(lambda x: ''.join(re.sub(r\"[^a-zA-Z0-9]+\", ' ', i) for i in x ))\r\n",
        "#Removal of StopWords\r\n",
        "stop = stopwords. words('english')\r\n",
        "data_frame['removed stopwords'] = data_frame['NLP Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\r\n",
        "print(data_frame)\r\n",
        "#Stemming Data\r\n",
        "st = PorterStemmer()\r\n",
        "data_frame['Stemming'] = data_frame['removed stopwords'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\r\n",
        "#Lemmatization Data\r\n",
        "nltk.download('wordnet')\r\n",
        "data_frame['Lemmatization'] = data_frame['NLP Text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\r\n",
        "data_frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiV6Y-V-u4-A",
        "outputId": "d7c2cfe8-e307-45b2-89ec-2b380228f8f2"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "\n",
            "Nothing to update.\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "\n",
            "Commands:\n",
            "  d) Download a package or collection     u) Update out of date packages\n",
            "  l) List packages & collections          h) Help\n",
            "  c) View & Modify Configuration          q) Quit\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDXc4_uwvT7K"
      },
      "source": [
        "parts_of_speech = []\r\n",
        "for sentence in data_frame['NLP Text']:\r\n",
        "  parts_of_speech.append(nltk.pos_tag(word_tokenize(sentence)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRbwcNk8v4T0"
      },
      "source": [
        "dict_counts = {}\r\n",
        "Final_dict = {}\r\n",
        "Final_dict['Cordinating Conjuction'] = []\r\n",
        "Final_dict['Determiner'] = []\r\n",
        "Final_dict['Modal'] = []\r\n",
        "Final_dict['Preposition'] = []\r\n",
        "Final_dict['Adjective'] = []\r\n",
        "Final_dict['Proper Noun'] = []\r\n",
        "Final_dict['Verb'] = []\r\n",
        "Final_dict['Possessive Ending'] = []\r\n",
        "Final_dict['Possesive Pronoun'] = []\r\n",
        "for i in parts_of_speech:\r\n",
        "  dict_counts['CC'] = dict_counts['DT'] = dict_counts['MD'] = dict_counts['IN'] = dict_counts['JJS'] = dict_counts['NNP'] = dict_counts['VB'] = dict_counts['POS'] = dict_counts['PRP$'] = 0\r\n",
        "  for j in i:\r\n",
        "    if j[1] == 'CC':\r\n",
        "      dict_counts['CC'] += 1\r\n",
        "    elif j[1] == 'DT':\r\n",
        "      dict_counts['DT'] += 1\r\n",
        "    elif j[1] == 'MD':\r\n",
        "      dict_counts['MD'] += 1\r\n",
        "    elif j[1] == 'IN':\r\n",
        "      dict_counts['IN'] += 1\r\n",
        "    elif j[1] == 'JJS':\r\n",
        "      dict_counts['JJS'] += 1\r\n",
        "    elif j[1] == 'NNP':\r\n",
        "      dict_counts['NNP'] += 1\r\n",
        "    elif j[1] == 'VB':\r\n",
        "      dict_counts['VB'] += 1\r\n",
        "    elif j[1] == 'POS':\r\n",
        "      dict_counts['POS'] += 1\r\n",
        "    elif j[1] == 'PRP$':\r\n",
        "      dict_counts['PRP$'] += 1\r\n",
        "  Final_dict['Cordinating Conjuction'].append(dict_counts['CC'])\r\n",
        "  Final_dict['Determiner'].append(dict_counts['DT'])\r\n",
        "  Final_dict['Modal'].append(dict_counts['MD'])\r\n",
        "  Final_dict['Preposition'].append(dict_counts['IN'])\r\n",
        "  Final_dict['Adjective'].append(dict_counts['JJS'])\r\n",
        "  Final_dict['Proper Noun'].append(dict_counts['NNP'])\r\n",
        "  Final_dict['Verb'].append(dict_counts['VB'])\r\n",
        "  Final_dict['Possessive Ending'].append(dict_counts['POS'])\r\n",
        "  Final_dict['Possesive Pronoun'].append(dict_counts['PRP$'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZn2lriiwCLN"
      },
      "source": [
        "features_dataframe = pd.DataFrame.from_dict(Final_dict)\r\n",
        "print(features_dataframe)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}